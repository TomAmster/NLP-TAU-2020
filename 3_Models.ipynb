{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3. Models.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM4ntpUeKJBa+halRq3Z8mn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TomAmster/NLP-TAU-2020/blob/master/3_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x3VTTCVzjMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.init import xavier_uniform_ as xavier_uniform\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from math import floor, sqrt, sin, cos\n",
        "import random\n",
        "import sys\n",
        "import time\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9i-gnR8mKr9",
        "colab_type": "text"
      },
      "source": [
        "# 1.Define Neural Network Model and Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE6nUofso2rC",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 Set Up Base Model Embedding Layer\n",
        "\n",
        "\n",
        "1.   Define Embedding Size - We Use 300D Vectors\n",
        "2.   Create an Embedding Layer - We Use Glove Embedding Matrix\n",
        "3.   Define Loss Function - Cross Entropy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQYmzxRGzpYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaseModel(nn.Module):\n",
        "\n",
        "    def __init__(self, Y, word_embeddings_matrix, vocab_size, dropout=0.5, gpu=True, embed_size, embed_freeze=False, hier=False):\n",
        "        super(BaseModel, self).__init__()\n",
        "        torch.manual_seed(1337)\n",
        "        self.gpu = gpu\n",
        "        self.hier = hier\n",
        "        self.Y = Y\n",
        "        self.embed_size = embed_size\n",
        "        self.embed_drop = nn.Dropout(p=dropout)\n",
        "        \n",
        "        print('loading model')\n",
        "\n",
        "        #make embedding layer\n",
        "        if word_embeddings_matrix is not None:               \n",
        "            W = torch.Tensor(word_embeddings_matrix)\n",
        "            self.embed = nn.Embedding.from_pretrained(W, freeze=embed_freeze)\n",
        "            self.embed.padding_idx = 0\n",
        "        else:\n",
        "            self.embed = nn.Embedding(vocab_size+2, embed_size, padding_idx=0)\n",
        "\n",
        "    def _get_loss(self, yhat, target):\n",
        "        return F.binary_cross_entropy_with_logits(yhat, target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMi7f2Ssrth1",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Set Up Base Model Attention Layer\n",
        "\n",
        "1. If Embedding Descriptions - Using Description Data and Attention\n",
        "2. Else,  Creating a Uniform Embedding Matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdLRbZEkruIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(torch.nn.Module):\n",
        "    def __init__(self, n_dim, n_labels, embed_desc=False, desc_dim=300):\n",
        "\n",
        "        super(Attention, self).__init__()\n",
        "        \n",
        "        self.embed_desc = embed_desc\n",
        "        self.softmax = nn.Softmax(dim=2)\n",
        "        \n",
        "        if self.embed_desc:\n",
        "            self.linear1 = nn.Linear(desc_dim, n_dim)\n",
        "            self.linear2 = nn.Linear(n_dim, n_dim)\n",
        "            xavier_uniform(self.linear1.weight)\n",
        "            xavier_uniform(self.linear2.weight)\n",
        "            self.activation = nn.Tanh()\n",
        "\n",
        "        else:\n",
        "            self.U = nn.Parameter(torch.FloatTensor(n_labels, n_dim))\n",
        "            xavier_uniform(self.U)\n",
        "\n",
        "    def forward(self, x, desc_data=None):\n",
        "\n",
        "        if self.embed_desc:\n",
        "            desc_data = self.activation(self.linear1(desc_data))\n",
        "            desc_data, _ = desc_data.max(dim=1, keepdim=False)\n",
        "            desc_data = self.activation(self.linear2(desc_data))\n",
        "            alpha = self.softmax(desc_data.matmul(x.transpose(1,2)))\n",
        "        else:\n",
        "            alpha = self.softmax(self.U.matmul(x.transpose(1,2)))\n",
        "\n",
        "        return alpha.matmul(x), alpha\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9OGjZ_xzxr_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvEncoder(nn.Module):\n",
        "    def __init__(self, n_dim_in, n_dim_out, kernel_size, padding=True):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv1d(n_dim_in, n_dim_out, kernel_size=kernel_size, padding=floor(kernel_size/2) if padding == True else False)\n",
        "        xavier_uniform(self.conv.weight)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.conv(x.transpose(1,2)).transpose(1,2)\n",
        "  \n",
        "c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkpZcznjz7fX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvAttnPool(BaseModel):\n",
        "\n",
        "    def __init__(self, Y, dims, kernel_size, embed_matrix, gpu, vocab_size, Y_coarse=None, embed_size=100, embed_freeze=False, dropout=[0.5], hier=False, embed_desc=False, layer_norm=False, fine2coarse = None):\n",
        "        super(ConvAttnPool, self).__init__(Y, embed_matrix, vocab_size, dropout=dropout[0], gpu=gpu, embed_size=dims[0], embed_freeze=embed_freeze, hier=hier)\n",
        "        \n",
        "        self.conv = ConvEncoder(dims[0], dims[1], kernel_size, padding=True)\n",
        "        \n",
        "        self.activation = nn.Tanh()\n",
        "        \n",
        "        self.attention = Attention(dims[1], Y, embed_desc, desc_dim=self.embed_size)\n",
        "        \n",
        "        self.layer_norm = nn.LayerNorm(torch.Size([dims[1]])) if layer_norm else None\n",
        "        \n",
        "        self.embed_desc = embed_desc\n",
        "        \n",
        "        self.final = nn.Linear(dims[1], Y, bias=True)\n",
        "        xavier_uniform(self.final.weight)\n",
        "        \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "        if self.hier:\n",
        "            self.attention_coarse = Attention(dims[1], Y_coarse)\n",
        "            xavier_uniform(self.attention_coarse.weight)\n",
        "        \n",
        "            self.final_coarse = nn.Linear(dims[1], Y_coarse, bias=True)\n",
        "            xavier_uniform(self.final_coarse.weight)\n",
        "            \n",
        "            if fine2coarse is not None:\n",
        "                self.fine2coarse = torch.LongTensor(fine2coarse)\n",
        "\n",
        "    def forward(self, x, target, target_coarse=None, desc_data=None, get_attention=True):\n",
        "        \n",
        "        x = self.embed(x)\n",
        "        \n",
        "        x = self.embed_drop(x)\n",
        "        \n",
        "        x = self.activation(self.conv(x))\n",
        "        \n",
        "        if self.layer_norm is not None:\n",
        "            x = self.layer_norm(x)\n",
        "        \n",
        "        if self.hier:\n",
        "            m_coarse, alpha_coarse = self.attention_coarse(x)\n",
        "            yhat_coarse = self.final_coarse.weight.mul(m_coarse).sum(dim=2).add(self.final_coarse.bias)\n",
        "            \n",
        "            m, alpha = self.attention(x, self.embed(desc_data)) if self.embed_desc else self.attention(x)\n",
        "            yhat = self.final.weight.mul(m).sum(dim=2).add(self.final.bias)\n",
        "            \n",
        "            mask = torch.round(self.sigmoid(yhat_coarse[:,self.fine2coarse]))\n",
        "            yhat = yhat * mask\n",
        "            \n",
        "            loss = self._get_loss(yhat, target) + self._get_loss(yhat_coarse, target_coarse)\n",
        "            \n",
        "            yhat_coarse = self.sigmoid(yhat_coarse)\n",
        "            yhat = self.sigmoid(yhat)\n",
        "            \n",
        "            return (yhat, yhat_coarse), loss, (alpha, alpha_coarse)\n",
        "        else:    \n",
        "            m, alpha = self.attention(x, self.embed(desc_data)) if self.embed_desc else self.attention(x)\n",
        "            yhat = self.final.weight.mul(m).sum(dim=2).add(self.final.bias)\n",
        "            \n",
        "            loss = self._get_loss(yhat, target)\n",
        "            yhat = self.sigmoid(yhat)\n",
        "            \n",
        "            return yhat, loss, alpha"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EGvifulz9pH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvDilated(BaseModel):\n",
        "\n",
        "    def __init__(self, Y, dims, kernel_size, dilation, embed_matrix, gpu, vocab_size, Y_coarse=None, embed_size=100, embed_freeze=False, dropout=[0.5], hier=False, embed_desc=False, fine2coarse = None):\n",
        "        super(ConvDilated, self).__init__(Y, embed_matrix, vocab_size, dropout=.0, gpu=gpu, embed_size=dims[0], embed_freeze=embed_freeze, hier=hier)\n",
        "        \n",
        "        assert len(dropout) == len(dims)-1\n",
        "        \n",
        "        self.drops = nn.ModuleList([nn.Dropout(p=drop) for drop in dropout])\n",
        "        \n",
        "        self.convs = nn.ModuleList([nn.Conv1d(dims[i], dims[i+1], kernel_size=kernel_size, dilation=dilation[i], padding=floor(kernel_size/2)*dilation[i]) for i in range(len(self.drops))])\n",
        "        \n",
        "        self.layer_norms = nn.ModuleList([nn.LayerNorm(torch.Size([dims[i+1]])) for i in range(0,len(self.drops))])\n",
        "        \n",
        "        self.activation = nn.Tanh()\n",
        "        \n",
        "        self.attention = Attention(dims[-1], Y, embed_desc, desc_dim=self.embed_size)\n",
        "        \n",
        "        self.embed_desc = embed_desc\n",
        "        \n",
        "        self.final = nn.Linear(dims[-1], Y, bias=True)\n",
        "        xavier_uniform(self.final.weight)\n",
        "        \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "        if self.hier:\n",
        "            self.attention_coarse = Attention(dims[-1], Y_coarse)\n",
        "            xavier_uniform(self.attention_coarse.weight)\n",
        "        \n",
        "            self.final_coarse = nn.Linear(dims[-1], Y_coarse, bias=True)\n",
        "            xavier_uniform(self.final_coarse.weight)\n",
        "            \n",
        "            if fine2coarse is not None:\n",
        "                self.fine2coarse = torch.LongTensor(fine2coarse)\n",
        "\n",
        "    def forward(self, x, target, target_coarse=None, desc_data=None, get_attention=True):\n",
        "        \n",
        "        x = self.embed(x)\n",
        "        \n",
        "        for i in range(len(self.convs)):\n",
        "            x = self.drops[i](x)\n",
        "            x = x.transpose(1,2)\n",
        "            x = self.convs[i](x)\n",
        "            x = x.transpose(1,2)\n",
        "            x = self.activation(x)\n",
        "            x = self.layer_norms[i](x)\n",
        "\n",
        "        if self.hier:\n",
        "            m_coarse, alpha_coarse = self.attention_coarse(x)\n",
        "            yhat_coarse = self.final_coarse.weight.mul(m_coarse).sum(dim=2).add(self.final_coarse.bias)\n",
        "            \n",
        "            m, alpha = self.attention(x, self.embed(desc_data)) if self.embed_desc else self.attention(x)\n",
        "            yhat = self.final.weight.mul(m).sum(dim=2).add(self.final.bias)\n",
        "            \n",
        "            mask = torch.round(self.sigmoid(yhat_coarse[:,self.fine2coarse]))\n",
        "            yhat = yhat * mask\n",
        "            \n",
        "            loss = self._get_loss(yhat, target) + self._get_loss(yhat_coarse, target_coarse)\n",
        "            \n",
        "            yhat_coarse = self.sigmoid(yhat_coarse)\n",
        "            yhat = self.sigmoid(yhat)\n",
        "            \n",
        "            return (yhat, yhat_coarse), loss, (alpha, alpha_coarse)\n",
        "        else:    \n",
        "            m, alpha = self.attention(x, self.embed(desc_data)) if self.embed_desc else self.attention(x)\n",
        "            yhat = self.final.weight.mul(m).sum(dim=2).add(self.final.bias)\n",
        "            \n",
        "            loss = self._get_loss(yhat, target)\n",
        "            yhat = self.sigmoid(yhat)\n",
        "            \n",
        "            return yhat, loss, alpha"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVm2QAnGz_xf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}